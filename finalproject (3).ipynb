{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKNmiuGhM2mt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  #იმპორტები\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "!pip install evaluate\n",
        "!pip install -q transformers datasets evaluate gradio accelerate scikit-learn matplotlib\n",
        "import evaluate\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "dc-p1rluM-67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Arseniy-Sandalov/Georgian-Sentiment-Analysis\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "ZzBtrsYZNPZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # ვწმენდ დატას და ვშლი ლინკებს\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # ზედმეტი სფეისების წაშლა\n",
        "    return text\n",
        "\n",
        "dataset = dataset.map(lambda x: {\"text\": clean_text(x[\"text\"])})"
      ],
      "metadata": {
        "id": "rVbF3003NXOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0] #შევამოწმე მუშაობა"
      ],
      "metadata": {
        "id": "YAT4MlnoNagU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda x: x[\"sentiment\"] in [\"Negative\", \"Neutral\", \"Positive\"])"
      ],
      "metadata": {
        "id": "57UUxDbrNdwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_no_mixed = dataset.filter(   ## ამოვშალე mixed ტექსტი\n",
        "    lambda x: x[\"sentiment\"] in [\"Negative\", \"Neutral\", \"Positive\"]\n",
        ")"
      ],
      "metadata": {
        "id": "w2Ws1gd4NgON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter  #შევამოწმე\n",
        "\n",
        "labels = [ex[\"sentiment\"] for ex in dataset_no_mixed['train']]\n",
        "Counter(labels)"
      ],
      "metadata": {
        "id": "mEOhxeOvNipr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_labels(example): # მივანიჭე Labels\n",
        "    return example['sentiment'] in [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "dataset['train'] = dataset['train'].filter(filter_labels)"
      ],
      "metadata": {
        "id": "PYQFOBzWNlWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\"Negative\":0, \"Neutral\":1, \"Positive\":2}\n",
        "def encode_labels(example):\n",
        "    example[\"label\"] = label_mapping[example[\"sentiment\"]]\n",
        "    return example"
      ],
      "metadata": {
        "id": "r80GzuIaNpgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "czOJfAAxNsOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "_4tlKgFfNua0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "RTBmbg0ENw7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_data = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "train_dataset = split_data[\"train\"]\n",
        "test_dataset = split_data[\"test\"]\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
        "\n",
        "model_name = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "FzJvA3rjN0hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Arseniy-Sandalov/Georgian-Sentiment-Analysis\")\n",
        "\n",
        "print(dataset['train'].column_names)"
      ],
      "metadata": {
        "id": "HOLZu7iIPgY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "raw_dataset = load_dataset(\"Arseniy-Sandalov/Georgian-Sentiment-Analysis\")\n",
        "\n",
        "df = pd.DataFrame(raw_dataset['train'])\n",
        "\n",
        "df = df.rename(columns={'sentiment': 'label'})\n",
        "df = df[['text', 'label']]\n",
        "\n",
        "from datasets import Dataset\n",
        "full_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "dataset_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "test_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(\"დატასეტი მზადაა\", train_dataset.column_names)"
      ],
      "metadata": {
        "id": "R35ogG5XP7O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "def map_labels(example):\n",
        "    if isinstance(example[\"label\"], str):\n",
        "        return {\"label\": label_map.get(example[\"label\"].lower(), 1)}\n",
        "    return {\"label\": int(example[\"label\"])}\n",
        "\n",
        "train_dataset = train_dataset.map(map_labels)\n",
        "test_dataset = test_dataset.map(map_labels)\n",
        "\n",
        "print(\"ლეიბლები გადაკონვერტირდა ციფრებში\")"
      ],
      "metadata": {
        "id": "7wy53_WqRhNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(example):\n",
        "    val = example[\"label\"]\n",
        "\n",
        "    if isinstance(val, int):\n",
        "        return {\"label\": val}\n",
        "\n",
        "    mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "    if str(val).isdigit():\n",
        "        return {\"label\": int(val)}\n",
        "\n",
        "    return {\"label\": mapping.get(str(val).lower(), 1)}\n",
        "\n",
        "train_dataset = train_dataset.map(encode_labels)\n",
        "test_dataset = test_dataset.map(encode_labels)\n",
        "\n",
        "print(type(train_dataset[0]['label']))"
      ],
      "metadata": {
        "id": "cvmGMt7OSPuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "HwSdk9z0Q9Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "LCPU94AiQ-Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "labels = train_dataset[\"label\"]\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"weights_tensor შეიქმნა\")\n",
        "print(\"წონებია:\", weights_tensor)"
      ],
      "metadata": {
        "id": "Xs0WLAxxSxYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ვითვლით წონებს\n",
        "labels = train_dataset[\"label\"]\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2Padh8hHN3tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "7SiNJ_jxRKD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import Counter\n",
        "print(\"არსებული ლეიბლები:\", Counter(dataset_split['train']['label']))\n",
        "\n",
        "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "def map_labels(example):\n",
        "    val = str(example[\"label\"]).lower()\n",
        "    return {\"label\": label_map.get(val, 1)}\n",
        "\n",
        "train_ds = dataset_split['train'].map(map_labels)\n",
        "\n",
        "negatives = train_ds.filter(lambda x: x['label'] == 0)\n",
        "neutrals  = train_ds.filter(lambda x: x['label'] == 1)\n",
        "positives = train_ds.filter(lambda x: x['label'] == 2)\n",
        "\n",
        "print(f\"ნეგატიური: {len(negatives)}, ნეიტრალური: {len(neutrals)}, პოზიტიური: {len(positives)}\")"
      ],
      "metadata": {
        "id": "uCuMSMZjULat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from accelerate import Accelerator\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "Accelerator().free_memory()\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3)\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "GneLBTMwVQda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# ჩავტვირთე accuracy-ს მეტრიკა\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "iNymbdV1VhgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"final_model\")\n",
        "tokenizer.save_pretrained(\"final_model\")"
      ],
      "metadata": {
        "id": "UsdICIavVVtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "# ვამოწმებ შეცდომებს რომ accuracy ავამაღლო\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "wrong_preds = []\n",
        "id2label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "\n",
        "print(\"შეცდომების ძებნა...\")\n",
        "\n",
        "for i in tqdm(range(len(tokenized_test))):\n",
        "\n",
        "    inputs = {\n",
        "        \"input_ids\": tokenized_test[i][\"input_ids\"].unsqueeze(0).to(device),\n",
        "        \"attention_mask\": tokenized_test[i][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "    }\n",
        "\n",
        "    true_label = tokenized_test[i][\"label\"].item() # .item() სჭირდება, რომ რიცხვი ამოვიღოთ\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    pred_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    if pred_label != true_label:\n",
        "        text = tokenizer.decode(tokenized_test[i][\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "        wrong_preds.append({\n",
        "            \"Text\": text,\n",
        "            \"True Label\": id2label[true_label],\n",
        "            \"Predicted Label\": id2label[pred_label]\n",
        "        })\n",
        "\n",
        "df_errors = pd.DataFrame(wrong_preds)\n",
        "\n",
        "print(f\"\\nსულ ნაპოვნია {len(df_errors)} შეცდომა.\")\n",
        "print(\"-\" * 50)\n",
        "if len(df_errors) > 0:\n",
        "    display(df_errors.head(10))\n",
        "else:\n",
        "    print(\"არცერთი შეცდომა არ უპოვია)\")"
      ],
      "metadata": {
        "id": "wK6-P79fWEds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "input_ids = inputs['input_ids'].to(device)\n",
        "attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "prediction = torch.argmax(logits, dim=1).item()"
      ],
      "metadata": {
        "id": "O6PYwQQIogBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "for i in range(len(tokenized_test)):\n",
        "    input_ids = tokenized_test[i][\"input_ids\"].unsqueeze(0).to(device)\n",
        "    attention_mask = tokenized_test[i][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "    all_preds.append(torch.argmax(logits, dim=1).item())\n",
        "    all_labels.append(tokenized_test[i][\"label\"].item())\n",
        "\n",
        "# მატრიცის აგება\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('მოდელის ნათქვამი (Predicted)')\n",
        "plt.ylabel('სინამდვილე (True)')\n",
        "plt.title('Confusion Matrix - სად ეშლება მოდელს?')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_c2VrJ9eX8Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./final_model\"  #რეპოზიტორის სახელი\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    return {labels[i]: float(probabilities[0][i]) for i in range(3)}\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Textbox(placeholder=\"ჩაწერე კომენტარი აქ...\", lines=2),\n",
        "    outputs=gr.Label(num_top_classes=3),\n",
        "    title=\"ქართული სოციალური მედიის სენტიმენტ-ანალიზი\",\n",
        "    description=\"შეიყვანეთ ტექსტი, რათა გაიგოთ, არის ის პოზიტიური, ნეგატიური თუ ნეიტრალური.\"\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "OMNUryQWYScL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ვიგებ accuracy-ს\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = len(tokenized_test)\n",
        "\n",
        "print(\"სიზუსტის დათვლა\")\n",
        "\n",
        "for i in tqdm(range(total)):\n",
        "\n",
        "    inputs = {\n",
        "        \"input_ids\": tokenized_test[i][\"input_ids\"].unsqueeze(0).to(device),\n",
        "        \"attention_mask\": tokenized_test[i][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "    }\n",
        "    true_label = tokenized_test[i][\"label\"].item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    if prediction == true_label:\n",
        "        correct += 1\n",
        "\n",
        "#შედეგი გამომაქვს პროცენტებში\n",
        "accuracy = correct / total\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(f\"საბოლოო სიზუსტე (Accuracy): {accuracy:.2%}\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "id": "9kyZtD-ZYwcR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}